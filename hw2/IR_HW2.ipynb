{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request,sys,time\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import os.path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "همه = 0\n",
    "سیاسی = 1\n",
    "اخبار سلامت = 39\n",
    "واژه خانه = 43\n",
    "که بوذ و چه کرد = 44\n",
    "قرآن = 45\n",
    "اقتصادی = 4\n",
    "اجتماعی = 5\n",
    "ورزشی = 6\n",
    "سلامت = 25\n",
    "بین الملل = 3\n",
    "سیاست خارجی = 2\n",
    "فرهنگی هنری = 8\n",
    "محیط زیست = 29\n",
    "فناوری = 14\n",
    "علمی = 7\n",
    "حوادث = 9\n",
    "سرگرمی = 15\n",
    "عمومی = 21\n",
    "کاربران = 22\n",
    "کافه گفت و گو = 23\n",
    "سفر و تفریح = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next page1\n",
      "next page2\n",
      "3\n",
      "next page4\n",
      "5\n",
      "6\n",
      "7\n",
      "next page8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "def extract_news(start_date = '1401/01/01',end_date = '1401/01/28',subject_id = 0):\n",
    "    page_number = 1\n",
    "    url = 'https://www.asriran.com/fa/archive?service_id=1&sec_id=-1&cat_id='+str(subject_id)+'&rpp=100&from_date='+start_date+'&to_date='+end_date+'&p='\n",
    "    while True:\n",
    "        # check if the current page's data exists\n",
    "        if (os.path.isfile('.\\\\data'+str(page_number)+'.csv')):\n",
    "            print('there is already page '+str(page_number))\n",
    "            page_number += 1\n",
    "            continue\n",
    "\n",
    "        upperframe=[] \n",
    "        url1 = url + str(page_number)\n",
    "        page = requests.get(url1)\n",
    "        soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "        articles=soup.find_all('article',attrs={'vizhe_cv col-xs-12 col-sm-6'})\n",
    "        print('page '+ str(page_number))\n",
    "        if len(articles)==0 :\n",
    "            print('finished')\n",
    "            break\n",
    "        \n",
    "        for article in articles:\n",
    "            title = article.find(\"a\",attrs={'class':\"vizhe_title\"}).text.strip()\n",
    "            link = article.find(\"a\").get('href').strip()\n",
    "            link = 'https://www.asriran.com' + link\n",
    "\n",
    "            summary = article.find(\"div\",attrs={'class':\"vizhe_lead\"}).text.strip()\n",
    "            date = article.find(\"span\",attrs={'class':\"tarikh_archive\"}).text.strip()\n",
    "\n",
    "            url2 = link\n",
    "            page2 = requests.get(url2)\n",
    "            soup2 = BeautifulSoup(page2.text, \"html.parser\")\n",
    "\n",
    "            # IF THE PAGE IS GONE\n",
    "            ERROR = soup2.find(\"div\",attrs={'class':\"error_container\"})\n",
    "            if ERROR :\n",
    "                subject = 'NOT FOUND'\n",
    "                body = 'NOT FOUND'\n",
    "                news_id = 'NOT FOUND'\n",
    "                short_link = 'NOT FOUND'\n",
    "                continue # jump to the next article\n",
    "\n",
    "\n",
    "            subject = soup2.find(\"div\",attrs={'class':\"news_path\"}).find_all(\"a\")[-1].text\n",
    "\n",
    "            body = soup2.find(\"div\",attrs={'class':\"body\"}).text.strip()[:-155]\n",
    "\n",
    "            news_id = soup2.find(\"div\",attrs={'class':\"news_nav news_id_c\"}).text.strip()\n",
    "            news_id = int(re.findall(r'\\d+', news_id)[0])\n",
    "\n",
    "            short_link = soup2.find(\"div\",attrs={'class':\"short-link row\"}).find(\"a\").get('href').strip()\n",
    "            short_link = 'https://www.asriran.com' + short_link\n",
    "\n",
    "            \n",
    "            frame = [news_id,date,subject,title,summary,body,short_link]\n",
    "            upperframe.append(frame)\n",
    "\n",
    "        data=pd.DataFrame(upperframe, columns=['News_ID','Date','Subject','Title','Summary','Body','Short_link'])\n",
    "        data.to_csv('data'+str(page_number)+'.csv', encoding='utf-8', index=False)\n",
    "        page_number += 1\n",
    "\n",
    "    #data=pd.DataFrame(upperframe, columns=['News_ID','Date','Subject','Title','Summary','Body','Short_link'])\n",
    "\n",
    "    return print(\"DONE\")\n",
    "\n",
    "extract_news(start_date = '1401/01/01',end_date = '1401/01/28',subject_id = 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37ee99bff104f98bd1f5372e2ee364e0f3435400cb3778afef49450308d28634"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
