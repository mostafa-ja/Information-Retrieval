{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request,sys,time\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import os.path\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "همه = 0\n",
    "سیاسی = 1\n",
    "اخبار سلامت = 39\n",
    "واژه خانه = 43\n",
    "که بوذ و چه کرد = 44\n",
    "قرآن = 45\n",
    "اقتصادی = 4\n",
    "اجتماعی = 5\n",
    "ورزشی = 6\n",
    "سلامت = 25\n",
    "بین الملل = 3\n",
    "سیاست خارجی = 2\n",
    "فرهنگی هنری = 8\n",
    "محیط زیست = 29\n",
    "فناوری = 14\n",
    "علمی = 7\n",
    "حوادث = 9\n",
    "سرگرمی = 15\n",
    "عمومی = 21\n",
    "کاربران = 22\n",
    "کافه گفت و گو = 23\n",
    "سفر و تفریح = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is already page 1\n",
      "there is already page 2\n",
      "there is already page 3\n",
      "there is already page 4\n",
      "there is already page 5\n",
      "there is already page 6\n",
      "there is already page 7\n",
      "there is already page 8\n",
      "there is already page 9\n",
      "there is already page 10\n",
      "there is already page 11\n",
      "there is already page 12\n",
      "there is already page 13\n",
      "there is already page 14\n",
      "there is already page 15\n",
      "there is already page 16\n",
      "there is already page 17\n",
      "there is already page 18\n",
      "there is already page 19\n",
      "there is already page 20\n",
      "there is already page 21\n",
      "there is already page 22\n",
      "there is already page 23\n",
      "there is already page 24\n",
      "there is already page 25\n",
      "page 26\n",
      "there is already page 27\n",
      "there is already page 28\n",
      "there is already page 29\n",
      "there is already page 30\n",
      "there is already page 31\n",
      "there is already page 32\n",
      "there is already page 33\n",
      "there is already page 34\n",
      "there is already page 35\n",
      "there is already page 36\n",
      "there is already page 37\n",
      "there is already page 38\n",
      "finished\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "def extract_news(start_date = '1401/01/01',end_date = '1401/01/28',subject_id = 0):\n",
    "    page_number = 1\n",
    "    url = 'https://www.asriran.com/fa/archive?service_id=1&sec_id=-1&cat_id='+str(subject_id)+'&rpp=100&from_date='+start_date+'&to_date='+end_date+'&p='\n",
    "    while True:\n",
    "        # check if the current page's data exists\n",
    "        if (os.path.isfile('.\\\\data'+str(page_number)+'.csv')):\n",
    "            print('there is already page '+str(page_number))\n",
    "            page_number += 1\n",
    "            continue\n",
    "\n",
    "        upperframe=[] \n",
    "        url1 = url + str(page_number)\n",
    "        page = requests.get(url1)\n",
    "        soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "\n",
    "        articles=soup.find_all('article',attrs={'vizhe_cv col-xs-12 col-sm-6'})\n",
    "        if len(articles)==0 :\n",
    "            print('finished')\n",
    "            break\n",
    "        print('page '+ str(page_number))\n",
    "\n",
    "        for article in articles:\n",
    "            title = article.find(\"a\",attrs={'class':\"vizhe_title\"}).text.strip()\n",
    "            link = article.find(\"a\").get('href').strip()\n",
    "            link = 'https://www.asriran.com' + link\n",
    "\n",
    "            summary = article.find(\"div\",attrs={'class':\"vizhe_lead\"}).text.strip()\n",
    "            date = article.find(\"span\",attrs={'class':\"tarikh_archive\"}).text.strip()\n",
    "\n",
    "            url2 = link\n",
    "            page2 = requests.get(url2)\n",
    "            soup2 = BeautifulSoup(page2.text, \"html.parser\")\n",
    "\n",
    "            # IF THE PAGE IS GONE\n",
    "            ERROR = soup2.find(\"div\",attrs={'class':\"error_container\"})\n",
    "            if ERROR :\n",
    "                subject = ''\n",
    "                body = 'صفحه درخواستی شما موجود نمی باشد'\n",
    "                news_id = ''\n",
    "                short_link = ''\n",
    "                frame = [news_id,date,subject,title,summary,body,short_link]\n",
    "                upperframe.append(frame)\n",
    "                continue # jump to the next article\n",
    "\n",
    "\n",
    "            subject = soup2.find(\"div\",attrs={'class':\"news_path\"}).find_all(\"a\")[-1].text\n",
    "\n",
    "            body = soup2.find(\"div\",attrs={'class':\"body\"}).text.strip()[:-155]\n",
    "\n",
    "            news_id = soup2.find(\"div\",attrs={'class':\"news_nav news_id_c\"}).text.strip()\n",
    "            news_id = int(re.findall(r'\\d+', news_id)[0])\n",
    "\n",
    "            short_link = soup2.find(\"div\",attrs={'class':\"short-link row\"}).find(\"a\").get('href').strip()\n",
    "            short_link = 'https://www.asriran.com' + short_link\n",
    "\n",
    "            \n",
    "            frame = [news_id,date,subject,title,summary,body,short_link]\n",
    "            upperframe.append(frame)\n",
    "\n",
    "        data=pd.DataFrame(upperframe, columns=['News_ID','Date','Subject','Title','Summary','Body','Short_link'])\n",
    "        data.to_csv('data'+str(page_number)+'.csv', encoding='utf-8', index=False)\n",
    "        page_number += 1\n",
    "\n",
    "    #data=pd.DataFrame(upperframe, columns=['News_ID','Date','Subject','Title','Summary','Body','Short_link'])\n",
    "\n",
    "    return print(\"DONE\")\n",
    "\n",
    "extract_news(start_date = '1401/01/01',end_date = '1401/01/28',subject_id = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the path for joining multiple files\n",
    "files = os.path.join(\".\\\\\", \"data*.csv\")\n",
    "\n",
    "# list of merged files returned\n",
    "files = glob.glob(files)\n",
    "\n",
    "# joining files with concat and read_csv\n",
    "df = pd.concat(map(pd.read_csv, files), ignore_index=True)\n",
    "\n",
    "# convert df to csv\n",
    "df.to_csv('data.csv', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37ee99bff104f98bd1f5372e2ee364e0f3435400cb3778afef49450308d28634"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
